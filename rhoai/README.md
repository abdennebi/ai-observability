# OpenShift monitor AI Workloads

Models served with `KServe` from `OpenShift AI` will include a `podMonitor` in the same namespace.
You can view any metrics generated from the model serving runtime from the OpenShift Console:

```
openshift-console -> Observe -> Metrics
```

For other AI Workloads, add a [PodMonitor](./podmonitor-example.yaml) or [ServiceMonitor](./servicemonitor-example.yaml)
and then any metrics generated by the workload will be visible in the openshift-console, also.
