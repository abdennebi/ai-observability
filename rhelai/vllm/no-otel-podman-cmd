# NOTES: Run this, then, cd ~/llama-stack/distrubutions/remote-vllm && 
# https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html


export INFERENCE_PORT=8000
export INFERENCE_MODEL=meta-llama/Llama-3.1-8B-Instruct
#export INFERENCE_MODEL=meta-llama/Llama-3.1-3B-Instruct
HF_TOKEN=xxxxxxxxx

podman run --rm -d \
    --name vllm \
    --device nvidia.com/gpu=all \
    -v /home/cloud-user/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
    -p $INFERENCE_PORT:$INFERENCE_PORT \
    --network=host \
    -e VLLM_LOGGING_LEVEL=DEBUG \
    docker.io/vllm/vllm-openai:latest \
    --model $INFERENCE_MODEL \
    --port $INFERENCE_PORT
