podman run --rm -d --name vllm \
  --network=host --ipc=host \
  --device nvidia.com/gpu=all \
  --security-opt label=disable \
  -e VLLM_LOGGING_LEVEL=DEBUG \
  -e OTEL_SERVICE_NAME="rhelai-vllm" \
  -e OTEL_EXPORTER_OTLP_TRACES_INSECURE=true \
  -v /home/cloud-user/.local/share/ramalama:/ramalama \
  quay.io/sallyom/vllm:otlp \
    --model /var/home/cloud-user/.cache/instructlab/models/meta-llama/Llama-3.1-8B-Instruct \
    --served-model-name llama3 --tensor-parallel-size 2 --otlp-traces-endpoint 127.0.0.1:4317 --collect-detailed-traces all
  
# quay.io/sallyom/vllm:otlp is built from ./Containerfile  
